<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>分布式系统 on 骚铭科技的博客（建设中）</title>
    <link>http://yunyu-mr.github.io/topics/%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/</link>
    <description>Recent content in 分布式系统 on 骚铭科技的博客（建设中）</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-EN</language>
    <managingEditor>yourname@example.com (骚铭)</managingEditor>
    <webMaster>yourname@example.com (骚铭)</webMaster>
    <copyright>&lt;i class=&#39;fa fa-copyright&#39;&gt;&lt;/i&gt; 2016 &lt;i class=&#39;fa fa-mars&#39;&gt;&lt;/i&gt;骚铭科技 SM-Tech!</copyright>
    <lastBuildDate>Sat, 09 Jul 2016 16:26:29 +0800</lastBuildDate>
    <atom:link href="http://yunyu-mr.github.io/topics/%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>(MIT 6.824 Distributed System) Google File System</title>
      <link>http://yunyu-mr.github.io/cloud/gfs/</link>
      <pubDate>Sat, 09 Jul 2016 16:26:29 +0800</pubDate>
      <author>yourname@example.com (骚铭)</author>
      <guid>http://yunyu-mr.github.io/cloud/gfs/</guid>
      <description>

&lt;h1 id=&#34;google-file-system-gfs:ae09b45d9caec0f3c7195ad9c5fb36f3&#34;&gt;Google File System (GFS)&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;主题：性能、容错、一致性&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;参考论文：&lt;a href=&#34;http://nil.csail.mit.edu/6.824/2016/papers/gfs.pdf&#34;&gt;http://nil.csail.mit.edu/6.824/2016/papers/gfs.pdf&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;什么是一致性-consistency:ae09b45d9caec0f3c7195ad9c5fb36f3&#34;&gt;什么是一致性（consistency）？&lt;/h2&gt;

&lt;p&gt;当data是&lt;strong&gt;多副本的&lt;/strong&gt;和&lt;strong&gt;并发读写的&lt;/strong&gt;的时候，保持数据的一致性是非常重要的。&lt;/p&gt;

&lt;p&gt;弱一致性：read() 可能返回过期的数据（stale data）——不一定是最新的数据。&lt;/p&gt;

&lt;p&gt;强一致性：read() 返回最近一次 write() 的数据。&lt;/p&gt;

&lt;p&gt;权衡：strong consistency is good for applications&amp;rsquo; writers; but is bad for performence.&lt;/p&gt;

&lt;h2 id=&#34;理想的一致性模型:ae09b45d9caec0f3c7195ad9c5fb36f3&#34;&gt;理想的一致性模型&lt;/h2&gt;

&lt;p&gt;多副本文件系统表现得就像无副本文件系统一样（表现得就像：多用户access 同一台机器上的单个磁盘）。&lt;/p&gt;

&lt;p&gt;我们的目标就是要实现这种透明性。&lt;/p&gt;

&lt;h2 id=&#34;造成不一致的根源:ae09b45d9caec0f3c7195ad9c5fb36f3&#34;&gt;造成不一致的根源&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;并发（concurrency）&lt;/li&gt;
&lt;li&gt;错误（failure）&lt;/li&gt;
&lt;li&gt;网络分割（network partition）&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;gfs-的目标:ae09b45d9caec0f3c7195ad9c5fb36f3&#34;&gt;GFS 的目标？&lt;/h2&gt;

&lt;p&gt;创造一种基于集群的共享文件系统，存储超大规模数据集。（基于常用的Linux服务器集群，而不是大型机或超算）。&lt;/p&gt;

&lt;h2 id=&#34;gfs中的文件有什么性质:ae09b45d9caec0f3c7195ad9c5fb36f3&#34;&gt;GFS中的文件有什么性质？&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;数据集很&lt;strong&gt;大&lt;/strong&gt;（multi TB)&lt;/li&gt;
&lt;li&gt;系统中有不少&lt;strong&gt;大文件&lt;/strong&gt;（几百M甚至几个GB）&lt;/li&gt;
&lt;li&gt;append-only（many large, sequential writes that append to data files)&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;主要的挑战:ae09b45d9caec0f3c7195ad9c5fb36f3&#34;&gt;主要的挑战？&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;当集群规模很大时，机器崩溃(failures)变得十分频繁&lt;/li&gt;
&lt;li&gt;做到高性能：大量的并发读写&lt;/li&gt;
&lt;li&gt;如何提高网络利用率&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;顶层设计:ae09b45d9caec0f3c7195ad9c5fb36f3&#34;&gt;顶层设计&lt;/h2&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;为什么采用三个副本？&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;提高可用性(availability)&lt;/li&gt;
&lt;li&gt;负载均衡（如果数据经常被读，多副本可以均衡负载）&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;为什么不用RAID磁盘阵列？&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;RAID并不是一般常用品。而且我们要使得机器具有容错性，而不仅仅是存储设备。&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;为什么 chunks 设计得这么大？&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;减少clients 与 master 的交互次数。&lt;/li&gt;
&lt;li&gt;保持更长时间的TCP连接，减少网络开销。&lt;/li&gt;
&lt;li&gt;减少master 存储元数据（metadata）的空间。&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;master-如何做到容错:ae09b45d9caec0f3c7195ad9c5fb36f3&#34;&gt;Master 如何做到容错？&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;单一Master （论文2.4）&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;client 总是要跟 master 联系，这种中心化管理可以避免很多问题。master 可以将 client 的操作 log 下来，有利于恢复。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;持续性地(&lt;strong&gt;persistently&lt;/strong&gt;) 存储&lt;strong&gt;有限&lt;/strong&gt;的信息（论文2.6 metadata）&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;GFS的master 只存储最基本的文件和目录的元数据，可以有限做到分离。持续性地存储下面两类元数据：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;the file and chunk &lt;strong&gt;namespace&lt;/strong&gt; (目录)&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;strong&gt;file-to-chunk mapping&lt;/strong&gt;（文件到块的映射）&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;对上面两类数据的修改操作打&lt;strong&gt;log&lt;/strong&gt; （operation log）&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;log 要备份到多台远程机器上。&lt;/li&gt;
&lt;li&gt;仅当 log 顺利记录到本地和远程磁盘上后，才会回复client。&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;限制 log 的大小&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;定期建立 checkpoint，在检查点之前的旧日志会全部被删除。日志尽可能小，可以加快崩溃后的恢复速度。&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;恢复&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;首先将所有操作跑一遍（从上次检查点开始），恢复目录和映射。（replay operation logs after checkpoint）&lt;/li&gt;
&lt;li&gt;然后轮询chunkservers, 将chunk的位置信息恢复。&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;影子master （shadow master）（论文5.1.3）&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;master 除了有多个镜像外，还有&lt;strong&gt;影子&lt;/strong&gt;。影子跟镜像方式不太一样，影子是紧跟主master的步伐，慢一点，所以不完全相同，没那么新鲜；而镜像之间应该是完全相同的。影子可以提供“只读”服务，为master分担一些工作量，只是data没那么新鲜罢了。&lt;/p&gt;

&lt;h2 id=&#34;chunk-容错:ae09b45d9caec0f3c7195ad9c5fb36f3&#34;&gt;Chunk 容错&lt;/h2&gt;

&lt;p&gt;对chunk最多的操作是&lt;strong&gt;修改（mutation）&lt;/strong&gt;，那么如何避免在修改过程中的错误呢？&lt;/p&gt;

&lt;p&gt;GFS中主要采用&lt;strong&gt;租约机制&lt;/strong&gt;（lease，见论文3.1节）：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;首先client 会询问 master 哪个 chunk 握有当前租约（如果没有，就分配租约给其中一个副本），该chunk 就作为主chunk（primary）。&lt;/li&gt;
&lt;li&gt;client 将数据 push 到所有的副本中。注意，push 的过程是按照&lt;strong&gt;网络拓扑&lt;/strong&gt;传输的，而不是先传给primary再给secondary，这么做是为了以最快的速度将数据push到所有的副本中（最快速度拷贝）。&lt;/li&gt;
&lt;li&gt;当所有replicas 都接收完数据，client 就会向 primary 发出写入请求。

&lt;ul&gt;
&lt;li&gt;primary 为所有修改操作分配连续的序号（因为修改操作可能来自多个clients，所以必须保证执行顺序）&lt;/li&gt;
&lt;li&gt;primary 按照序号的顺序，对本地数据执行修改操作&lt;/li&gt;
&lt;li&gt;primary 将请求转发给其他的 replicas&lt;/li&gt;
&lt;li&gt;primary 收到所有 replicas 的回复后，才响应client，完成修改。&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;如果其中一个副本没有响应或修改失败，client 就会&lt;strong&gt;重试&lt;/strong&gt;。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;除了上述的租约机制，master 还会执行re-replication 和 rebalance （见论文4.3节）：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;当某个chunk 副本的数目低于设定值（默认为3），master 就会尽快对该chunk 进行再备份（re-replication）。&lt;/li&gt;
&lt;li&gt;master 还会根据 chunk 的访问情况对其副本进行&lt;strong&gt;负载均衡&lt;/strong&gt;，这是为了使磁盘利用率更为平均，提升性能。&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;chunks-的一致性:ae09b45d9caec0f3c7195ad9c5fb36f3&#34;&gt;Chunks 的一致性&lt;/h2&gt;

&lt;p&gt;GFS 如何保证所有chunks都是一致的呢？（见论文2.7.1节，4.5节）&lt;/p&gt;

&lt;p&gt;GFS 使用 chunk 版本号（chunk version number）来检测chunk 是否新鲜（stale）。&lt;/p&gt;

&lt;p&gt;当master分配一个租约给某个chunk时，它会增加版本号并通知所有的副本。master 和 chunkservers 都持续性地存储这个版本号。那么 master 如何检测到 chunk 是不新鲜的呢？一旦chunkserver崩溃重启，它会向master 汇报它的chunks的版本号。如果master 发现其中某个chunk的版本号比自己存储的版本号小，那么这个chunk肯定是不新鲜了。&lt;/p&gt;

&lt;p&gt;版本号还会发给client，使client也可以检测数据是否新鲜。&lt;/p&gt;

&lt;h2 id=&#34;并发-写入-追加-见论文3-3:ae09b45d9caec0f3c7195ad9c5fb36f3&#34;&gt;并发 写入/追加（见论文3.3）&lt;/h2&gt;

&lt;p&gt;在传统的“写入”中，通过偏移量offset 就可对数据进行随机读写。而在&lt;strong&gt;并发&lt;/strong&gt;情况下，常常会有&lt;strong&gt;多个client并发地&lt;/strong&gt;在&lt;strong&gt;不同机器&lt;/strong&gt;上对&lt;strong&gt;同一个文件&lt;/strong&gt;进行写入操作（例如，log 文件）。显然，传统的随机写入并不适合并发环境，因为要搞好竞争关系往往要引入复杂的&lt;strong&gt;锁&lt;/strong&gt;机制，会严重降低分布式系统的读写性能。所以，在GFS中，采用&lt;strong&gt;追加&lt;/strong&gt;的方式修改文件。&lt;/p&gt;

&lt;p&gt;GFS提供一种称为&lt;strong&gt;记录追加(record append)&lt;/strong&gt;的原子操作。此操作保证&lt;strong&gt;至少一次追加（at least once）&lt;/strong&gt;。为什么是至少一次呢？且看记录是如何追加的：&lt;/p&gt;

&lt;p&gt;追加也是一种修改，所以跟上述GFS的修改方式一样，必须要所有replicas都追加了，才算成功。只要有一个replica追加失败，client 就会&lt;strong&gt;重试&lt;/strong&gt;。可见，虽然某些replicas已经成功追加，但client的重试使得他们只得再加一次。所以，某些记录可能会被追加多次，但至少有一次。&lt;/p&gt;

&lt;h2 id=&#34;小结:ae09b45d9caec0f3c7195ad9c5fb36f3&#34;&gt;小结&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;GFS 中使用的重要容错技术：

&lt;ul&gt;
&lt;li&gt;日志和检查点（logging &amp;amp; checkpointing）&lt;/li&gt;
&lt;li&gt;chunk 采用&lt;strong&gt;主/备份&lt;/strong&gt;副本存储&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;GFS适合于什么？长处？

&lt;ul&gt;
&lt;li&gt;大规模&lt;strong&gt;连续&lt;/strong&gt;地读写&lt;/li&gt;
&lt;li&gt;追加（append）&lt;/li&gt;
&lt;li&gt;大吞吐量(三个副本供同时读取，并且将clients的读取&lt;strong&gt;聚合&lt;/strong&gt;起来，使得网络接近&lt;strong&gt;饱和&lt;/strong&gt;状态)&lt;/li&gt;
&lt;li&gt;数据容错性好（三个以上副本）&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;不足：

&lt;ul&gt;
&lt;li&gt;管理采用集中化模式（单master），master容错性能不够好。&lt;/li&gt;
&lt;li&gt;小文件（master是瓶颈，若clients大量读取小文件，每次都要先请求master，那么master的工作量就会激增）&lt;/li&gt;
&lt;li&gt;对于并发的文件更新还不够好（除了append）&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Raft -- Understandable Distributed Consensus</title>
      <link>http://yunyu-mr.github.io/cloud/raft/</link>
      <pubDate>Fri, 08 Jul 2016 09:59:36 +0800</pubDate>
      <author>yourname@example.com (骚铭)</author>
      <guid>http://yunyu-mr.github.io/cloud/raft/</guid>
      <description>

&lt;h2 id=&#34;简介&#34;&gt;简介&lt;/h2&gt;

&lt;p&gt;​   Raft 是一个便于理解的一致性算法(Consensus algorithm)，在此之前，最为著名的就是 Paxos 算法。很多一致性算法都是基于paxos的，但说实话，paxos真特么难以理解，看来看去只能看懂算法流程，无法理解为什么work。而Raft 要好理解的多。看动画演示可以理解大致算法流程和如何容错，具体细节可以看 Raft 论文。&lt;/p&gt;

&lt;p&gt;​   &lt;a href=&#34;thesecretlivesofdata.com/raft/&#34;&gt;动画演示&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;​   &lt;a href=&#34;http://nil.csail.mit.edu/6.824/2016/papers/raft-extended.pdf&#34;&gt;Raft paper&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;​   文章中对一致性算法的定义我觉得不错:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;​ Consensus algorithms allow a collection of machines to work as a coherent group that can survive the fail-ures of some of its members.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;​   一致性算法使得一个&lt;strong&gt;机群&lt;/strong&gt;可以像一个协同相关的&lt;strong&gt;小组&lt;/strong&gt;一样工作，即使在某些机器挂掉的情况下也能正常工作。&lt;/p&gt;

&lt;h3 id=&#34;关键问题-如何避免大脑分裂-split-brain&#34;&gt;关键问题：如何避免大脑分裂（split brain）?&lt;/h3&gt;

&lt;p&gt;要避免大脑分裂，需要避免两个问题：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Crashed：即使某个member挂了，整个机群仍然可以正常工作，也即容错。&lt;/li&gt;
&lt;li&gt;Partition：即使机群因为通信异常被分成两份，仍然可以工作，也即可以避免网络分割。&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;状态机&#34;&gt;状态机&lt;/h2&gt;

&lt;h2 id=&#34;算法流程&#34;&gt;算法流程&lt;/h2&gt;

&lt;p&gt;Raft 主要有两个步骤：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;选举：Leader election&lt;/li&gt;
&lt;li&gt;复制：Log replication&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;每个 member 可能有三种角色（状态）：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Leader&lt;/li&gt;
&lt;li&gt;Candidate&lt;/li&gt;
&lt;li&gt;Follower&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;leader-election&#34;&gt;Leader Election&lt;/h3&gt;

&lt;p&gt;Raft 中Leader的思想跟Paxos算法中的有点相似，第一步都是要确定一个Leader。那么怎么选举呢？这里涉及两个超时：election timeout 跟 vote timeout。&lt;/p&gt;

&lt;p&gt;首先，选举超时的 follower 会变成 candidate，然后他就会发送 RequestVote 请求。&lt;/p&gt;

&lt;p&gt;然后，进入选举模式，该 candidate 等待 大部分（majority）成员的响应。如果等了很久都没反应，这时投票超时，candidate 会再次发起选举请求；如果大部分成员及时回复Ok，那么candidate 就变成 leader。&lt;/p&gt;

&lt;p&gt;选出 leader 后，它必须周期性地发送 heartbeat 保持存在感。只要心跳及时回复，leader 就一直保持领导地位。一旦任意一个 follower 选举超时，就会重新选举。&lt;/p&gt;

&lt;h3 id=&#34;log-replication&#34;&gt;Log Replication&lt;/h3&gt;

&lt;p&gt;Client 把指令发给 leader， leader 负责备份指令。&lt;/p&gt;

&lt;p&gt;首先，client 将指令发给 leader，leader 把指令写入自己的 log 序列中。&lt;/p&gt;

&lt;p&gt;然后，leader 向 follower 发送 AppendEntry 请求（其实就是加在心跳中）。Follower 收到请求后，将指令写入自己的 log 序列中，并回复Ok。&lt;/p&gt;

&lt;p&gt;最后，如果 leader 得到大部分成员的通过，他就会将指令写入&lt;strong&gt;状态机&lt;/strong&gt;中，并回复 client。随后，leader 还要告诉其他 follower 也把刚才的指令写入状态机中；如果 leader 得不到部分成员的通过，他是不会将指令加入状态机的，也就是说这条指令没有大部分成员的通过，是不会最终执行的。&lt;/p&gt;

&lt;h2 id=&#34;课程问题&#34;&gt;课程问题&lt;/h2&gt;

&lt;h4 id=&#34;when-does-raft-start-a-leader-election&#34;&gt;When does Raft start a leader election?&lt;/h4&gt;

&lt;ol&gt;
&lt;li&gt;如果 follower 在election timeout 时间内没有收到 heartbeat，那么他就会变成 candidate，并发起选举。&lt;/li&gt;
&lt;li&gt;如果candidate 发出选举请求后，在限定投票时间内没收到足够多的票数，他会再次发起选举。&lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&#34;how-to-ensure-at-most-one-leader-in-a-term&#34;&gt;How to ensure at most one leader in a term?&lt;/h4&gt;

&lt;p&gt;前提：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;只有得到大部分成员（majority）的票数才能成为leader。&lt;/li&gt;
&lt;li&gt;每个 follower 在每个 term 期间只能投一次票。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;所以，在每个 term 期间，最多只有一个 leader 可以得到大部分成员的票数。&lt;/p&gt;

&lt;h4 id=&#34;how-does-a-server-know-that-election-succeeded&#34;&gt;How does a server know that election succeeded?&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;Candidate 获取 majority 票数时，他就知道自己获胜了。&lt;/li&gt;
&lt;li&gt;Follower 收到 AppendEntries 心跳时，他就知道新 leader 产生了。&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&#34;an-election-may-not-succeed&#34;&gt;An election may not succeed？&lt;/h4&gt;

&lt;p&gt;如果有&lt;strong&gt;大于3个&lt;/strong&gt; candidate 同时发起选举，那么是不可能产生 leader 的。不过这种情况出现的概率实在是太低啦！因为Raft 的超时时间是随机产生的，产生三个相同的随机数，你说概率低不低？&lt;/p&gt;

&lt;h4 id=&#34;what-happens-after-a-failed-election&#34;&gt;What happens after a failed election?&lt;/h4&gt;

&lt;p&gt;重新选呗！等待另一个 follower 超时，currentTerm ++，变成 candidate。currentTerm 越大则越新，老的 candidate 会被淘汰（不选他）。&lt;/p&gt;

&lt;h4 id=&#34;how-does-raft-reduce-chances-of-election-failure-due-to-split-vote&#34;&gt;How does Raft reduce chances of election failure due to split vote?&lt;/h4&gt;

&lt;p&gt;正如上面所说，Raft 的超时时间是&lt;strong&gt;随机&lt;/strong&gt;产生的，所以多个 candidate 同时超时然后发起选举的概率是非常低的。还有，根据论文&lt;strong&gt;5.6小节&lt;/strong&gt;，时间上还要满足下面这个时间条件：&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;broadcastTime ≪ electionTimeout ≪ MTBF&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;广播时间必须远远小于选举超时，否则如果两个超时随机数比较接近也不行，容易出现二次选举（前一次没完成，第二个超时又产生了）。&lt;/p&gt;

&lt;h4 id=&#34;how-to-choose-the-random-delay-range&#34;&gt;How to choose the random delay range?&lt;/h4&gt;

&lt;p&gt;太短：容易产生二次选举（如上所述，前一次选举没完成，第二个超时又来了）。&lt;/p&gt;

&lt;p&gt;太长：等待时间太长，浪费了，效率低。&lt;/p&gt;

&lt;h4 id=&#34;what-if-old-leader-isn-t-aware-a-new-one-is-elected&#34;&gt;What if old leader isn&amp;rsquo;t aware a new one is elected?&lt;/h4&gt;

&lt;p&gt;就算老 Leader 不知道新 leader 已经被选，也不影响大局。因为 majority 已经选择了新 leader，老leader 已被抛弃。即使老leader继续发 AppendEntries，也只有 minority 会回复ok，majority 是会拒绝他的，也就是说他的 log 不会写入状态机。&lt;/p&gt;

&lt;p&gt;内部分歧，但不会造成&amp;raquo;人格分裂“。&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;下面谈谈在出现错误之后，如何同步日志&lt;/p&gt;

&lt;h4 id=&#34;what-do-we-want-to-ensure&#34;&gt;What do we want to ensure?&lt;/h4&gt;

&lt;p&gt;我们希望每个server 执行的命令都相同，顺序相同，也即他们的状态是一致的。详见论文Fig.3&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;State Machine Safety&lt;/strong&gt;: if a server has applied a log entry at a given index to its state machine, no other server will ever apply a different log entry for the same index.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h4 id=&#34;how-can-logs-disagree&#34;&gt;How can logs disagree?&lt;/h4&gt;

&lt;p&gt;日志不同的情况有两种：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;缺失。&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;s0: 3&lt;/p&gt;

&lt;p&gt;s1: 3, 3, 3&lt;/p&gt;

&lt;p&gt;s2: 3, 3, 3&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;错乱。&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;s0: 3&lt;/p&gt;

&lt;p&gt;s1: 3, 3, 4&lt;/p&gt;

&lt;p&gt;s2: 3, 3, 5&lt;/p&gt;

&lt;h4 id=&#34;new-leader-will-force-its-log-on-followers&#34;&gt;New leader will force its log on followers&lt;/h4&gt;

&lt;p&gt;New leader 会先找到跟 followers 匹配的logs，然后将后面不匹配的全部删掉，并将logs补齐。&lt;/p&gt;

&lt;h4 id=&#34;could-new-leader-roll-back-executed-entries-from-end-of-previous-term&#34;&gt;Could new leader roll back &lt;em&gt;executed&lt;/em&gt; entries from end of previous term?&lt;/h4&gt;

&lt;p&gt;不可以，一旦执行是不能撤销的！否则将违反状态机安全性原则(state machine safty）。Raft 为了保证这一点，选举的新 leader 的日志里必须包含全部已经提交的记录（entries）。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Raft uses the voting process to prevent a candidate fromwinning an election unless its log contains all committedentries.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h4 id=&#34;could-we-choose-leader-with-longest-log&#34;&gt;Could we choose leader with longest log?&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;  example:
    S1: 5 6 7
    S2: 5 8
    S3: 5 8
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;即使 S1 有最长的日志，但他是不可能被选中的，因为他还停留在 term 7。而 S2 和 S3 已经到 term 8 了。所以，新 leader 只会在 S2和 S3中产生，选择包含最长日志的 candidate 是不科学的。&lt;/p&gt;

&lt;h4 id=&#34;please-explained-at-least-as-up-to-date-voting-rules&#34;&gt;Please explained ”&lt;em&gt;At least as up to date&lt;/em&gt;&amp;raquo; voting rules?&lt;/h4&gt;

&lt;ol&gt;
&lt;li&gt;优先选择有最新的日期（terms）候选者。&lt;/li&gt;
&lt;li&gt;如果记录中最新的terms 都相同，则优先选择日志长的候选者。&lt;/li&gt;
&lt;/ol&gt;

&lt;blockquote&gt;
&lt;p&gt;If the logs have last entries with different terms, thenthe log with the later term is more up-to-date.&lt;/p&gt;

&lt;p&gt;If the logs end with the same term, then whichever log is longer ismore up-to-date.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;hr /&gt;

&lt;p&gt;下面谈谈配置更改的问题&lt;/p&gt;

&lt;h4 id=&#34;why-doesn-t-a-straightforward-approach-work&#34;&gt;Why doesn&amp;rsquo;t a straightforward approach work?&lt;/h4&gt;

&lt;p&gt;下面这张图表示增加两个servers：S4, S5。也就是从3节点 -&amp;gt; 5节点。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://yunyu-mr.github.io/img/raft-reconfiguration.png&#34; alt=&#34;raft-reconfiguration&#34; /&gt;&lt;/p&gt;

&lt;p&gt;由于收到新配置的时间不同，会出现下面这个时刻：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;S1, S2 用旧配置（3个节点），那么，S1 可能会被选为leader（只需要S1,S2同意）；&lt;/li&gt;
&lt;li&gt;S3, S4, S5 使用新配置（5个节点），那么，S5 可能会被选为leader（只需要S3,S4,S5同意）。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;结果可能导致这个时刻出现两个 leader，显然，大脑分裂了！&lt;/p&gt;

&lt;h4 id=&#34;raft-configuration-change-how&#34;&gt;Raft configuration change. How?&lt;/h4&gt;

&lt;p&gt;思路：增加一个过渡阶段 “joint consensus”，同时包含旧配置$C_{old}$和新配置$C_{new}$（过渡配置：$C_{old,new}$）。&lt;/p&gt;

&lt;p&gt;步骤：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Leader 收到更改配置请求后，将$C_{old,new}$弄成一条日志记录（log entry），然后同步它。&lt;/li&gt;
&lt;li&gt;一旦$C_{old,new}$被提交，$C_{old}$就不再使用了，当然，$C_{new}$也不会使用。这保证了过渡时期的安全。随后，$C_{new}$会被发送。&lt;/li&gt;
&lt;li&gt;$C_{new}$被提交后，配置就更改成功了。&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;容错：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;如果发生崩溃，而新leader 还没收到 $C_{old,new}$，那么他会继续使用$C_{old}$，并没什么影响。&lt;/li&gt;
&lt;li&gt;如果崩溃后新选举的leader 已经有$C_{old,new}$记录了，那么他会继续完成配置更改。&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;疑问&#34;&gt;疑问&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;以前听云计算课程，一般replica 的数目为&lt;strong&gt;3&lt;/strong&gt;个或&lt;strong&gt;5&lt;/strong&gt;个。而在Raft中，一个cluster 是没有限定数目的，如果我的集群有100台集群，岂不是有100个replicas？这不科学吧，太浪费了。再说，如果面对超大规模的文件，如何处理？&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;​My answer: Raft 是一致性协议，是为了使所有机器的操作保持一致，而不是用于备份文件。&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>